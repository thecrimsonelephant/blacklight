{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "25ddb449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pprint as pp\n",
    "import requests\n",
    "import base64\n",
    "import hashlib\n",
    "import datetime as dt\n",
    "\n",
    "load_dotenv()\n",
    "apikey = os.getenv(\"APIKEY\")\n",
    "# data_path = os.path.join(os.getcwd(), 'data', 'urlhaus_malicious-urls.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "200fccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadHausJSON(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # pp.pprint(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6f0fd76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseHausData(data):\n",
    "    parsed = []\n",
    "    for key, entries in data.items():\n",
    "        for item in entries:\n",
    "            parsed.append({\n",
    "                \"id\": key,  # keep track of the original key\n",
    "                \"url\": item.get(\"url\"),\n",
    "                \"url_status\": item.get(\"url_status\"),\n",
    "                \"date_added\": item.get(\"dateadded\"),\n",
    "                \"last_online\": item.get(\"last_online\"),\n",
    "                \"tags\": item.get(\"tags\", []),\n",
    "                \"threat\": item.get(\"threat\"),\n",
    "                \"reporter\": item.get(\"reporter\"),\n",
    "                \"urlhaus_link\": item.get(\"urlhaus_link\")\n",
    "            })\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f9968e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getURLID(all):\n",
    "    df = pd.DataFrame(all) # storing parsed data in dataframe\n",
    "    print(df.head()) # for my sanity (#1)\n",
    "    urls = df['url'].head(1) # getting top 3 in dataframe (for testing before moving into rate limiting)\n",
    "    encoded = [] # for storing encoded IDs\n",
    "    print(urls) # printing urls for my sanity (#2) - ensuring they're the correct number!\n",
    "    for url in urls:\n",
    "        # encode URL as URL-safe base64 without trailing '=' padding as suggested in documentation\n",
    "        url_bytes = url.encode('utf-8') # encoding also suggested in docs\n",
    "        base64_bytes = base64.urlsafe_b64encode(url_bytes) # encoding\n",
    "        base64_str = base64_bytes.decode('utf-8').rstrip(\"=\") # stripping trailing '='\n",
    "        encoded.append(base64_str) # appending encoded to list\n",
    "    print(f'GETURLID --- {encoded}')\n",
    "    return encoded # returning list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4832221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(encodingURLIDs):   \n",
    "    # initializing empty lists for parsing... thought about doing json_normalize and then dropping, but keys would be appended inline rather than to individual lists, so opting for this\n",
    "\n",
    "    getURL = 'https://www.virustotal.com/api/v3/urls/' # setting up API URI to hit\n",
    "    # attributes\n",
    "    first_submission_date_unix = []\n",
    "    last_analysis_date_unix = []\n",
    "    first_submission_date = []\n",
    "    last_analysis_date = []\n",
    "    maliciousURL = []\n",
    "    threat_names = []\n",
    "\n",
    "    # last analysis results\n",
    "    names = []\n",
    "    categories = []\n",
    "    engine_names = []\n",
    "    methods = []\n",
    "    results = []\n",
    "\n",
    "    # setting up API headers with apikey\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"x-apikey\": apikey\n",
    "    }\n",
    "\n",
    "    # print(encodingURLIDs) # triple checking urlID list. For sanity (#3)\n",
    "\n",
    "    # looping through list of encoded URLs\n",
    "    for encodingURLID in encodingURLIDs:\n",
    "        # Get URL report (GET)\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                f\"{getURL}/{encodingURLID}\", # GET request to VT with encodedURLID\n",
    "                headers=headers,\n",
    "            )\n",
    "            r = response.json() # storing response in json\n",
    "\n",
    "            # print(r['data']['attributes'].keys()) # sanity check\n",
    "            attributes = r['data']['attributes'] # parsing json - first up, attributes key and vals!\n",
    "            last_analysis_results = r['data']['attributes']['last_analysis_results'] # getting last_analysis data\n",
    "\n",
    "            # print(last_analysis_results)\n",
    "            # not a nested list! So surfing the top with key/value and .items to get values\n",
    "            for key, value in last_analysis_results.items():\n",
    "                first_submission_date_unix.append(attributes['last_submission_date'])\n",
    "                last_analysis_date_unix.append(attributes['last_analysis_date'])\n",
    "                maliciousURL.append(attributes['url'])\n",
    "                threat_names.append(attributes['threat_names'])\n",
    "                # print(f\"Key: {key} --- Value: {value}\") # checking for fun* (*ahem, sanity. #4) \n",
    "                names.append(key)\n",
    "                methods.append(value['method'])\n",
    "                engine_names.append(value['engine_name'])\n",
    "                categories.append(value['category'])\n",
    "                results.append(value['result'])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    # massive sanity check #5 but this time it's valid because if any are incorrect lengths, they won't append... maybe try/except here.\n",
    "    print(len(first_submission_date_unix))\n",
    "    print(len(last_analysis_date_unix))\n",
    "    print(len(maliciousURL))\n",
    "    print(len(threat_names))\n",
    "    print(len(names))\n",
    "    print(len(categories))\n",
    "    print(len(engine_names))\n",
    "    print(len(methods))\n",
    "    print(len(results))\n",
    "\n",
    "    # storing data in dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'names': names,\n",
    "        'first_submission_date_unix' : first_submission_date_unix,\n",
    "        'last_analysis_date_unix' : last_analysis_date_unix,\n",
    "        'malicious_url' : maliciousURL,\n",
    "        'threat_names' : threat_names,\n",
    "        'methods' : methods,\n",
    "        'engine_names' : engine_names,\n",
    "        'categories' : categories,\n",
    "        'results': results\n",
    "\n",
    "    })\n",
    "    # adding sha256 uniqueID here for postgres storing\n",
    "    df['composite_key'] = df['names'].astype(str) + '-' + df['malicious_url'].astype(str) # creating a column of the string version of the eventual sha256 encoded ID is faster and easier than using a function and looping through the dataframe, especially for large DFs!\n",
    "    df['unique_id'] = [hashlib.sha256(x.encode()).hexdigest() for x in df['composite_key']]\n",
    "    print(\"Created Unique IDs\") # just for clarification\n",
    "    \n",
    "    # editing the unixtimestamps in the date fields to actual dates, and then saving to dataframe\n",
    "    for unix_timestamp in df['first_submission_date_unix']:\n",
    "        print(unix_timestamp)\n",
    "        dates = dt.datetime.fromtimestamp(unix_timestamp)\n",
    "        first_submission_date.append(dates)\n",
    "    for unix_timestamp in df['last_analysis_date_unix']:\n",
    "        print(unix_timestamp)\n",
    "        dates = dt.datetime.fromtimestamp(unix_timestamp)\n",
    "        last_analysis_date.append(dates)\n",
    "    df['first_submission_date'] = first_submission_date\n",
    "    df['last_analysis_date'] = last_analysis_date\n",
    "    colstodrop = ['composite_key', 'first_submission_date_unix', 'last_analysis_date_unix'] # setting up columns to drop\n",
    "    df = df.drop(columns=colstodrop) # dropping columns to drop (see line 97)\n",
    "    df = df[sorted(df.columns)] # sorting because my brain breaks if not\n",
    "    # print(df) # sanity (promise it's the last one: #6)\n",
    "    return df # celebrate ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
